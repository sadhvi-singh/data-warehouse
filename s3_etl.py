# -*- coding: utf-8 -*-
"""s3_etl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/107H6iPwiYFmbl_k12dZi8gIqHrUDIih_
"""

#!pip install apache-airflow -q
#!pip install snowflake-connector-python -q

#!pip install apache-airflow-providers-snowflake -q

#pip install --ignore-installed blinker

from airflow import DAG
from airflow.models import Variable
from airflow.decorators import task
from airflow.operators.python import get_current_context

from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook

from datetime import datetime
from datetime import timedelta
import logging
import snowflake.connector

def return_snowflake_conn():

    # Initialize the SnowflakeHook
    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')

    # Execute the query and fetch results
    conn = hook.get_conn()
    return conn.cursor()

@task
def create_tables(con, table1, table2):
  try:
    con.execute("BEGIN")
    con.execute(f"""CREATE TABLE IF NOT EXISTS {table1} (
    userId int not NULL,
    sessionId varchar(32) primary key,
    channel varchar(32) default 'direct'
        )""")
    con.execute(f"""CREATE TABLE IF NOT EXISTS {table2} (
    sessionId varchar(32) primary key,
    ts timestamp
        )""")
    con.execute("COMMIT")
    return table1,table2
  except Exception as e:
        con.execute("ROLLBACK")
        print(e)
        raise(e)

@task
def load_data(tables, url):
  try:
    con.execute("BEGIN")
    con.execute(f"""CREATE OR REPLACE STAGE dev.raw_data.blob_stage
    url = {url}
    file_format = (type = csv, skip_header = 1, field_optionally_enclosed_by = '"');""")
    con.execute(f"""COPY INTO {table1} FROM @dev.raw_data.blob_stage/user_session_channel.csv; """)
    con.execute(f"""COPY INTO {table2} FROM @dev.raw_data.blob_stage/session_timestamp.csv; """)
    con.execute("COMMIT")
  except Exception as e:
        con.execute("ROLLBACK")
        print(e)
        raise(e)

with DAG (
    dag_id = 'etl_s3',
    description = 'ETL Pipeline from Amazon s3 to Snowflake',
    start_date = datetime(2024,10,6),
    catchup=False,
    tags=['ETL'],
    schedule = '30 15 * * *'
) as dag:
    con = return_snowflake_conn()
    url = Variable.get("url")
    table1 = "dev.raw_data.user_session_channel"
    table2 = "dev.raw_data.session_timestamp"

    tables = create_tables(con, table1, table2)
    load = load_data(tables, url)